{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPMlb49kB5V+b7tMMd9uEdW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArushiG11/Anime-Data-Analysis/blob/main/Anime_Dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/manami-project/anime-offline-database?tab=readme-ov-file"
      ],
      "metadata": {
        "id": "l9fEB_D_ubd0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sPZjYO2VpowH"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import pandas as pd\n",
        "import os, re"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "aniSearch : https://www.anisearch.com/anime/toplist"
      ],
      "metadata": {
        "id": "egE_hx7vbfub"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "animes = []\n",
        "for pg in range(1,51):\n",
        "  url_as = f'https://www.anisearch.com/anime/toplist/page-{pg}'\n",
        "  header = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.75 Safari/537.36\",\n",
        "    \"X-Requested-With\": \"XMLHttpRequest\"\n",
        "  }\n",
        "  page_as = requests.get(url_as, headers=header)\n",
        "  soup = BeautifulSoup(page_as.text,'html')\n",
        "  ul = soup.find('ul',class_ = 'covers fullsizeA')\n",
        "  items = ul.find_all('li')\n",
        "\n",
        "  for item in items:\n",
        "    anime = item.find('span', class_='details')\n",
        "    type_num_date = anime.find('span', class_='date').text\n",
        "    type_num_date = type_num_date.split()\n",
        "    anime_type = type_num_date[0][:-1]\n",
        "    num_episodes = type_num_date[1]\n",
        "    year = type_num_date[2][1:-1]\n",
        "    title = anime.find('span', class_='title').text\n",
        "    company_span = anime.find('span', class_='company')\n",
        "    company = company_span.text if company_span else None\n",
        "    rank = item.find('div',class_=\"rank\").text[1:]\n",
        "    rank = int(rank)\n",
        "    ratings = item.find('div',class_='star0').attrs['title']\n",
        "    ratings = ratings.split()\n",
        "    rating = ratings[0]\n",
        "    comment = ratings[1]\n",
        "    rating = rating.replace(',','')\n",
        "    rating = float(rating)\n",
        "    genre = item.find('div', class_='genre').text\n",
        "    animes.append([rank, title, year, num_episodes, genre, anime_type, company, rating, comment])\n",
        "df_as = pd.DataFrame(animes,columns=['Rank', 'Title', 'Year', 'Number of Episodes', 'Genre', 'Anime Type', 'Production House', 'Ratings', 'Comments'])"
      ],
      "metadata": {
        "collapsed": true,
        "id": "1xQEpoqGbWSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BoxOffice Mojo: https://www.boxofficemojo.com/genre/sg4259246337/, \\\n",
        "              https://www.boxofficemojo.com/genre/sg4259246337/?offset=100"
      ],
      "metadata": {
        "id": "gyyrFzVqpnA3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "urls_boxof = [\"https://www.boxofficemojo.com/genre/sg4259246337/\", \\\n",
        "              \"https://www.boxofficemojo.com/genre/sg4259246337/?offset=100\"]\n",
        "page_boxof = []\n",
        "for idx,i in enumerate(urls_boxof):\n",
        "    r_boxof = requests.get(i)\n",
        "    page_boxof.append(pd.read_html(r_boxof.text))\n",
        "    print(\"Boxoffice page is scanned: \" + str(idx+1) + \"/\" + str(len(urls_boxof)), end='\\r')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHHTpaLFpzOw",
        "outputId": "12cd6fdf-b57d-4f34-e904-4c61001b0517"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-3e5491320460>:6: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
            "  page_boxof.append(pd.read_html(r_boxof.text))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-3e5491320460>:6: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
            "  page_boxof.append(pd.read_html(r_boxof.text))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# boxoffice data\n",
        "# observe the shapes\n",
        "print(\"Shapes\", page_boxof[0][0].shape, page_boxof[1][0].shape)\n",
        "\n",
        "# combine databases obtained for each page\n",
        "df_boxof = pd.concat([page_boxof[0][0], page_boxof[1][0]], axis=0, ignore_index=True)\n",
        "\n",
        "# observe the new dataframe details\n",
        "df_boxof.info(verbose=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmqaciEqwwdW",
        "outputId": "cf4a825d-87d3-4186-e4b9-07299b43dcd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shapes (100, 8) (26, 8)\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 126 entries, 0 to 125\n",
            "Data columns (total 8 columns):\n",
            " #   Column          Non-Null Count  Dtype \n",
            "---  ------          --------------  ----- \n",
            " 0   Rank            126 non-null    int64 \n",
            " 1   Title           126 non-null    object\n",
            " 2   Lifetime Gross  126 non-null    object\n",
            " 3   Max Theaters    126 non-null    object\n",
            " 4   Opening         126 non-null    object\n",
            " 5   Open Th         126 non-null    object\n",
            " 6   Release Date    126 non-null    object\n",
            " 7   Distributor     126 non-null    object\n",
            "dtypes: int64(1), object(7)\n",
            "memory usage: 8.0+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "My Anime List : https://myanimelist.net/topanime.php"
      ],
      "metadata": {
        "id": "lsfG4qNr-Jcn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "urls_mal = ['https://myanimelist.net/topanime.php?limit',\n",
        "            'https://myanimelist.net/topanime.php?limit=50',\n",
        "            'https://myanimelist.net/topanime.php?limit=100',\n",
        "            'https://myanimelist.net/topanime.php?limit=150',\n",
        "            'https://myanimelist.net/topanime.php?limit=200',\n",
        "            'https://myanimelist.net/topanime.php?limit=250']\n",
        "\n",
        "pages_mal = []\n",
        "for idx,i in enumerate(urls_mal):\n",
        "    r_mal = requests.get(i)\n",
        "    pages_mal.append(BeautifulSoup(r_mal.text,'html'))"
      ],
      "metadata": {
        "id": "Q8v_cTvu1ARz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "anime_list = []\n",
        "\n",
        "for page in pages_mal:\n",
        "    table = page.find('table', class_='top-ranking-table')\n",
        "    if not table:\n",
        "        break\n",
        "    table_data = table.find_all('tr', class_='ranking-list')\n",
        "\n",
        "    for row in table_data:\n",
        "        rank = row.find('td', class_='rank ac').text\n",
        "        rank = int(rank)\n",
        "        title_cell = row.find('td', class_='title al va-t word-break')\n",
        "        link_cell = title_cell.find('a')\n",
        "        href = link_cell.get('href')\n",
        "        if href:\n",
        "            detail_response = requests.get(href)\n",
        "            detail_soup = BeautifulSoup(detail_response.content, 'html.parser')\n",
        "            Name_element = detail_soup.find('h1', class_='title-name h1_bold_none')\n",
        "            Name = Name_element.text.strip() if Name_element else 'N/A'\n",
        "            english_name_element = detail_soup.find('p', class_='title-english title-inherit')\n",
        "            English_Name = english_name_element.text.strip() if english_name_element else 'N/A'\n",
        "            Synopsis_element = detail_soup.find('p', itemprop='description')\n",
        "            Synopsis = Synopsis_element.text.strip() if Synopsis_element else 'N/A'\n",
        "            score_element = detail_soup.find('div', class_='fl-l score')\n",
        "            score = score_element.find('div').text.strip() if score_element else '0'\n",
        "            score = float(score)\n",
        "            Information_element = detail_soup.find_all('div', class_='spaceit_pad')\n",
        "            Information = {\n",
        "                'Type': 'N/A',\n",
        "                'Episodes': 0,\n",
        "                'Status': 'N/A',\n",
        "                'Aired': 'N/A',\n",
        "                'Season': 'N/A',\n",
        "                'Studios': 'N/A',\n",
        "                'Producers': 'N/A',\n",
        "                'Genres': 'N/A',\n",
        "                'Demographics': 'N/A',\n",
        "                'Source': 'N/A',\n",
        "                'Popularity': 0,\n",
        "                'Members': 'N/A'\n",
        "            }\n",
        "\n",
        "            for element in Information_element:\n",
        "                text = element.text.strip()\n",
        "                if 'Type:' in text:\n",
        "                    Information['Type'] = element.find('a').text.strip()\n",
        "                elif 'Episodes:' in text:\n",
        "                    Information['Episodes'] = text.split(':')[-1].strip()\n",
        "                elif 'Status:' in text:\n",
        "                    Information['Status'] = text.split(':')[-1].strip()\n",
        "                elif 'Aired:' in text:\n",
        "                    Information['Aired'] = text.split(':')[-1].strip()\n",
        "                elif 'Premiered:' in text:\n",
        "                    Information['Season'] = text.split(':')[-1].strip()\n",
        "                elif 'Studios:' in text:\n",
        "                    Information['Studios'] = element.find('a').text.strip()\n",
        "                elif 'Producers:' in text:\n",
        "                    Information['Producers'] = element.find('a').text.strip()\n",
        "                elif 'Genres:' in text:\n",
        "                    Information['Genres'] = element.find('a').text.strip()\n",
        "                elif 'Demographics:' in text:\n",
        "                    Information['Demographics'] = element.find('a').text.strip()\n",
        "                elif 'Source:' in text:\n",
        "                    Information['Source'] = text.split(':')[-1].strip()\n",
        "                elif 'Popularity:' in text:\n",
        "                    Information['Popularity'] = int(text.split(':')[-1].strip().replace('#', ''))\n",
        "                elif 'Members:' in text:\n",
        "                    Information['Members'] = text.split(':')[-1].strip()\n",
        "\n",
        "            anime_list.append([rank, Name, English_Name, Synopsis, score, Information['Popularity'], Information['Type'],\n",
        "                               Information['Episodes'], Information['Status'], Information['Aired'], Information['Season'],\n",
        "                               Information['Studios'], Information['Producers'], Information['Genres'], Information['Demographics'],\n",
        "                               Information['Members'], Information['Source']])\n",
        "\n",
        "df_mal = pd.DataFrame(anime_list, columns=['Rank', 'Name', 'English Name', 'Synopsis', 'Score', 'Popularity', 'Type',\n",
        "                                           'Number of Episodes', 'Status', 'Aired', 'Season', 'Studios', 'Producers', 'Genres', 'Demographics', 'Members', 'Source'])\n"
      ],
      "metadata": {
        "id": "7iEUpR0l7pl6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "limit = 0\n",
        "anime_list = []\n",
        "\n",
        "while limit <= 200:\n",
        "    urls_mal = f'https://myanimelist.net/topanime.php?limit={200}'\n",
        "    page = requests.get(urls_mal)\n",
        "    soup = BeautifulSoup(page.text, 'html.parser')\n",
        "    table = soup.find('table', class_='top-ranking-table')\n",
        "    if not table:\n",
        "        print(f\"No table found at limit {limit}\")\n",
        "        break\n",
        "    table_data = table.find_all('tr', class_='ranking-list')\n",
        "\n",
        "    for row in table_data:\n",
        "        try:\n",
        "            rank = int(row.find('td', class_='rank ac').text.strip())\n",
        "            title_cell = row.find('td', class_='title al va-t word-break')\n",
        "            link_cell = title_cell.find('a')\n",
        "            href = link_cell.get('href')\n",
        "            if href:\n",
        "                detail_response = requests.get(href)\n",
        "                detail_soup = BeautifulSoup(detail_response.content, 'html.parser')\n",
        "                Name_element = detail_soup.find('h1', class_='title-name h1_bold_none')\n",
        "                Name = Name_element.text.strip() if Name_element else 'N/A'\n",
        "                english_name_element = detail_soup.find('p', class_='title-english title-inherit')\n",
        "                English_Name = english_name_element.text.strip() if english_name_element else 'N/A'\n",
        "                Synopsis_element = detail_soup.find('p', itemprop='description')\n",
        "                Synopsis = Synopsis_element.text.strip() if Synopsis_element else 'N/A'\n",
        "                score_element = detail_soup.find('div', class_='fl-l score')\n",
        "                score = float(score_element.find('div').text.strip()) if score_element else 0.0\n",
        "                Information_element = detail_soup.find_all('div', class_='spaceit_pad')\n",
        "                Information = {\n",
        "                    'Type': 'N/A',\n",
        "                    'Episodes': 0,\n",
        "                    'Status': 'N/A',\n",
        "                    'Aired': 'N/A',\n",
        "                    'Season': 'N/A',\n",
        "                    'Studios': 'N/A',\n",
        "                    'Producers': 'N/A',\n",
        "                    'Genres': 'N/A',\n",
        "                    'Demographics': 'N/A',\n",
        "                    'Source': 'N/A',\n",
        "                    'Popularity': 0,\n",
        "                    'Members': 'N/A'\n",
        "                }\n",
        "\n",
        "                for element in Information_element:\n",
        "                    text = element.text.strip()\n",
        "                    if 'Type:' in text:\n",
        "                        Information['Type'] = element.find('a').text.strip()\n",
        "                    elif 'Episodes:' in text:\n",
        "                        Information['Episodes'] = text.split(':')[-1].strip()\n",
        "                    elif 'Status:' in text:\n",
        "                        Information['Status'] = text.split(':')[-1].strip()\n",
        "                    elif 'Aired:' in text:\n",
        "                        Information['Aired'] = text.split(':')[-1].strip()\n",
        "                    elif 'Premiered:' in text:\n",
        "                        Information['Season'] = text.split(':')[-1].strip()\n",
        "                    elif 'Studios:' in text:\n",
        "                        Information['Studios'] = element.find('a').text.strip()\n",
        "                    elif 'Producers:' in text:\n",
        "                        Information['Producers'] = element.find('a').text.strip()\n",
        "                    elif 'Genres:' in text:\n",
        "                        Information['Genres'] = element.find('a').text.strip()\n",
        "                    elif 'Demographics:' in text:\n",
        "                        Information['Demographics'] = element.find('a').text.strip()\n",
        "                    elif 'Source:' in text:\n",
        "                        Information['Source'] = text.split(':')[-1].strip()\n",
        "                    elif 'Popularity:' in text:\n",
        "                        Information['Popularity'] = int(text.split(':')[-1].strip().replace('#', ''))\n",
        "                    elif 'Members:' in text:\n",
        "                        Information['Members'] = text.split(':')[-1].strip()\n",
        "\n",
        "                anime_list.append([rank, Name, English_Name, Synopsis, score, Information['Popularity'], Information['Type'],\n",
        "                                   Information['Episodes'], Information['Status'], Information['Aired'], Information['Season'],\n",
        "                                   Information['Studios'], Information['Producers'], Information['Genres'], Information['Demographics'],\n",
        "                                   Information['Members'], Information['Source']])\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing row: {e}\")\n",
        "\n",
        "    limit += 50\n",
        "    time.sleep(1)  # Add delay to avoid rate limiting\n",
        "\n",
        "df_mal = pd.DataFrame(anime_list, columns=['Rank', 'Name', 'English Name', 'Synopsis', 'Score', 'Popularity', 'Type',\n",
        "                                           'Number of Episodes', 'Status', 'Aired', 'Season', 'Studios', 'Producers', 'Genres', 'Demographics', 'Members', 'Source'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOsT7_NWgUfF",
        "outputId": "b5c9e3d7-5bab-4c30-8d7b-f85d648a1e85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No table found at limit 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save raw data for emergency uses\n",
        "if not os.path.exists('data'):\n",
        "    os.mkdir('data')\n",
        "df_boxof.to_csv(os.path.join('data', 'raw_boxof.csv'), encoding='utf-8-sig')\n",
        "df_as.to_csv(os.path.join('data','aniSearch.csv'), encoding='utf-8-sig')\n",
        "df_mal.to_csv(os.path.join('data','MyAnimeList.csv'), encoding='utf-8-sig')"
      ],
      "metadata": {
        "id": "1M4uaM2tpUEx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}