{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOa21hbltS7+byUAj+HeP8Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArushiG11/Anime-Data-Analysis/blob/main/Anime_Data_scraping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/manami-project/anime-offline-database?tab=readme-ov-file"
      ],
      "metadata": {
        "id": "l9fEB_D_ubd0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sPZjYO2VpowH"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import pandas as pd\n",
        "import os, re"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "aniSearch : https://www.anisearch.com/anime/toplist"
      ],
      "metadata": {
        "id": "egE_hx7vbfub"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "animes = []\n",
        "for pg in range(1,51):\n",
        "  url_as = f'https://www.anisearch.com/anime/toplist/page-{pg}'\n",
        "  header = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.75 Safari/537.36\",\n",
        "    \"X-Requested-With\": \"XMLHttpRequest\"\n",
        "  }\n",
        "  page_as = requests.get(url_as, headers=header)\n",
        "  soup = BeautifulSoup(page_as.text,'html')\n",
        "  ul = soup.find('ul',class_ = 'covers fullsizeA')\n",
        "  items = ul.find_all('li')\n",
        "\n",
        "  for item in items:\n",
        "    anime = item.find('span', class_='details')\n",
        "    type_num_date = anime.find('span', class_='date').text\n",
        "    type_num_date = type_num_date.split()\n",
        "    anime_type = type_num_date[0][:-1]\n",
        "    num_episodes = type_num_date[1]\n",
        "    year = type_num_date[2][1:-1]\n",
        "    title = anime.find('span', class_='title').text\n",
        "    company_span = anime.find('span', class_='company')\n",
        "    company = company_span.text if company_span else None\n",
        "    rank = item.find('div',class_=\"rank\").text[1:]\n",
        "    rank = int(rank)\n",
        "    ratings = item.find('div',class_='star0').attrs['title']\n",
        "    ratings = ratings.split()\n",
        "    rating = ratings[0]\n",
        "    comment = ratings[1]\n",
        "    rating = rating.replace(',','')\n",
        "    rating = float(rating)\n",
        "    genre = item.find('div', class_='genre').text\n",
        "    animes.append([rank, title, year, num_episodes, genre, anime_type, company, rating, comment])\n",
        "df_as = pd.DataFrame(animes,columns=['Rank', 'Title', 'Year', 'Number of Episodes', 'Genre', 'Anime Type', 'Production House', 'Ratings', 'Comments'])"
      ],
      "metadata": {
        "collapsed": true,
        "id": "1xQEpoqGbWSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BoxOffice Mojo: https://www.boxofficemojo.com/genre/sg4259246337/, \\\n",
        "              https://www.boxofficemojo.com/genre/sg4259246337/?offset=100"
      ],
      "metadata": {
        "id": "gyyrFzVqpnA3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "urls_boxof = [\"https://www.boxofficemojo.com/genre/sg4259246337/\", \\\n",
        "              \"https://www.boxofficemojo.com/genre/sg4259246337/?offset=100\"]\n",
        "page_boxof = []\n",
        "for idx,i in enumerate(urls_boxof):\n",
        "    r_boxof = requests.get(i)\n",
        "    page_boxof.append(pd.read_html(r_boxof.text))\n",
        "    print(\"Boxoffice page is scanned: \" + str(idx+1) + \"/\" + str(len(urls_boxof)), end='\\r')"
      ],
      "metadata": {
        "id": "cHHTpaLFpzOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# boxoffice data\n",
        "# observe the shapes\n",
        "print(\"Shapes\", page_boxof[0][0].shape, page_boxof[1][0].shape)\n",
        "\n",
        "# combine databases obtained for each page\n",
        "df_boxof = pd.concat([page_boxof[0][0], page_boxof[1][0]], axis=0, ignore_index=True)\n",
        "\n",
        "# observe the new dataframe details\n",
        "df_boxof.info(verbose=True)"
      ],
      "metadata": {
        "id": "dmqaciEqwwdW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "My Anime List : https://myanimelist.net/topanime.php"
      ],
      "metadata": {
        "id": "lsfG4qNr-Jcn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "import random\n",
        "\n",
        "def get_page_with_retries(session, url, max_retries=5, initial_delay=1):\n",
        "    for i in range(max_retries):\n",
        "        try:\n",
        "            response = session.get(url)\n",
        "            response.raise_for_status()\n",
        "            return response\n",
        "        except requests.RequestException as e:\n",
        "            print(f\"Error fetching {url}: {e}\")\n",
        "            if i < max_retries - 1:\n",
        "                delay = initial_delay * (2 ** i) + random.uniform(0, 1)\n",
        "                print(f\"Retrying in {delay:.2f} seconds...\")\n",
        "                time.sleep(delay)\n",
        "    return None\n",
        "\n",
        "def scrape_anime_data(max_limit=200):\n",
        "    session = requests.Session()\n",
        "    session.headers.update({\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "    })\n",
        "\n",
        "    anime_list = []\n",
        "    for limit in range(0, max_limit + 1, 50):\n",
        "        urls_mal = f'https://myanimelist.net/topanime.php?limit={limit}'\n",
        "        page = get_page_with_retries(session, urls_mal)\n",
        "        if not page:\n",
        "            print(f\"Failed to fetch page at limit {limit}\")\n",
        "            continue\n",
        "\n",
        "        print(f\"Limit: {limit}, Status Code: {page.status_code}\")\n",
        "        soup = BeautifulSoup(page.text, 'html.parser')\n",
        "        table = soup.find('table', class_='top-ranking-table')\n",
        "        if not table:\n",
        "            print(f\"No table found at limit {limit}\")\n",
        "            continue\n",
        "        table_data = table.find_all('tr', class_='ranking-list')\n",
        "\n",
        "        for row in table_data:\n",
        "            try:\n",
        "                rank = int(row.find('td', class_='rank ac').text.strip())\n",
        "                title_cell = row.find('td', class_='title al va-t word-break')\n",
        "                link_cell = title_cell.find('a')\n",
        "                href = link_cell.get('href')\n",
        "                if href:\n",
        "                    detail_response = get_page_with_retries(session, href)\n",
        "                    if not detail_response:\n",
        "                        print(f\"Failed to fetch details for anime at {href}\")\n",
        "                        continue\n",
        "                    detail_soup = BeautifulSoup(detail_response.content, 'html.parser')\n",
        "\n",
        "                    # Extract information (same as before)\n",
        "                    Name_element = detail_soup.find('h1', class_='title-name h1_bold_none')\n",
        "                    Name = Name_element.text.strip() if Name_element else 'N/A'\n",
        "                    english_name_element = detail_soup.find('p', class_='title-english title-inherit')\n",
        "                    English_Name = english_name_element.text.strip() if english_name_element else 'N/A'\n",
        "                    Synopsis_element = detail_soup.find('p', itemprop='description')\n",
        "                    Synopsis = Synopsis_element.text.strip() if Synopsis_element else 'N/A'\n",
        "                    score_element = detail_soup.find('div', class_='fl-l score')\n",
        "                    score = float(score_element.find('div').text.strip()) if score_element else 0.0\n",
        "                    Information_element = detail_soup.find_all('div', class_='spaceit_pad')\n",
        "                    Information = {\n",
        "                        'Type': 'N/A',\n",
        "                        'Episodes': 0,\n",
        "                        'Status': 'N/A',\n",
        "                        'Aired': 'N/A',\n",
        "                        'Season': 'N/A',\n",
        "                        'Studios': 'N/A',\n",
        "                        'Producers': 'N/A',\n",
        "                        'Genres': 'N/A',\n",
        "                        'Demographics': 'N/A',\n",
        "                        'Source': 'N/A',\n",
        "                        'Popularity': 0,\n",
        "                        'Members': 'N/A'\n",
        "                    }\n",
        "\n",
        "                    for element in Information_element:\n",
        "                        text = element.text.strip()\n",
        "                        if 'Type:' in text:\n",
        "                            Information['Type'] = element.find('a').text.strip()\n",
        "                        elif 'Episodes:' in text:\n",
        "                            Information['Episodes'] = text.split(':')[-1].strip()\n",
        "                        elif 'Status:' in text:\n",
        "                            Information['Status'] = text.split(':')[-1].strip()\n",
        "                        elif 'Aired:' in text:\n",
        "                            Information['Aired'] = text.split(':')[-1].strip()\n",
        "                        elif 'Premiered:' in text:\n",
        "                            Information['Season'] = text.split(':')[-1].strip()\n",
        "                        elif 'Studios:' in text:\n",
        "                            Information['Studios'] = element.find('a').text.strip()\n",
        "                        elif 'Producers:' in text:\n",
        "                            Information['Producers'] = element.find('a').text.strip()\n",
        "                        elif 'Genres:' in text:\n",
        "                            Information['Genres'] = element.find('a').text.strip()\n",
        "                        elif 'Demographic:' in text:\n",
        "                            demographics_span = element.find('span', itemprop='genre')\n",
        "                            Information['Demographics'] = demographics_span.text.strip() if demographics_span else 'N/A'\n",
        "                        elif 'Source:' in text:\n",
        "                            Information['Source'] = text.split(':')[-1].strip()\n",
        "                        elif 'Popularity:' in text:\n",
        "                            Information['Popularity'] = int(text.split(':')[-1].strip().replace('#', ''))\n",
        "                        elif 'Members:' in text:\n",
        "                            Information['Members'] = text.split(':')[-1].strip()\n",
        "\n",
        "                    anime_list.append([rank, Name, English_Name, Synopsis, score, Information['Popularity'], Information['Type'],\n",
        "                                      Information['Episodes'], Information['Status'], Information['Aired'], Information['Season'],\n",
        "                                      Information['Studios'], Information['Producers'], Information['Genres'], Information['Demographics'],\n",
        "                                      Information['Members'], Information['Source']])\n",
        "\n",
        "                    # Add a small delay between requests to individual anime pages\n",
        "                    time.sleep(random.uniform(1, 3))\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing row: {e}\")\n",
        "\n",
        "        # Add a longer delay between main page requests\n",
        "        delay = random.uniform(5, 10)\n",
        "        print(f\"Sleeping for {delay:.2f} seconds...\")\n",
        "        time.sleep(delay)\n",
        "\n",
        "    df_mal = pd.DataFrame(anime_list, columns=['Rank', 'Name', 'English Name', 'Synopsis', 'Score', 'Popularity', 'Type',\n",
        "                                                            'Number of Episodes', 'Status', 'Aired', 'Season', 'Studios',\n",
        "                                                            'Producers', 'Genres', 'Demographics', 'Members', 'Source'])\n",
        "\n",
        "    return df_mal\n",
        "\n",
        "df_mal = scrape_anime_data(max_limit=200)"
      ],
      "metadata": {
        "id": "xIXzzKqQxLQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save raw data for emergency uses\n",
        "if not os.path.exists('data'):\n",
        "    os.mkdir('data')\n",
        "df_boxof.to_csv(os.path.join('data', 'raw_boxof.csv'), encoding='utf-8-sig')\n",
        "df_as.to_csv(os.path.join('data','aniSearch.csv'), encoding='utf-8-sig')\n",
        "df_mal.to_csv(os.path.join('data','MyAnimeList.csv'), encoding='utf-8-sig')"
      ],
      "metadata": {
        "id": "1M4uaM2tpUEx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}